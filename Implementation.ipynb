{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract tracksUrl, AlbumUrl, ArtistUrl, playlist name, NO. of tracks, pid fields\n",
    "\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "import os\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "artists = list()\n",
    "albums = list()\n",
    "count_names = 0\n",
    "count_tracks = 0\n",
    "for fname in os.listdir('F:\\\\SpotifyPlaylistData\\\\MPD\\\\data'):    \n",
    "    f = open(os.path.join('F:\\\\SpotifyPlaylistData\\\\MPD\\\\data', fname))\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    print(fname)\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        del artists[:]\n",
    "        del albums[:]\n",
    "        \n",
    "        if 'name' in slice['playlists'][playlist_id]:\n",
    "            name = slice['playlists'][playlist_id]['name']\n",
    "            playlist_name = ' '.join(name.lower().split())\n",
    "            count_names += 1         \n",
    "        playlist['name'] = playlist_name    \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid'] \n",
    "        playlist['pid'] = pid     \n",
    "\n",
    "        num_tracks = slice['playlists'][playlist_id]['num_tracks']      \n",
    "        playlist['num_tracks'] = num_tracks\n",
    "        if len(slice['playlists'][playlist_id]['tracks']) != num_tracks:\n",
    "            print('num_tracks mismatch!!')\n",
    "        \n",
    "        for track_id in range(num_tracks):\n",
    "            tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            artists.append(slice['playlists'][playlist_id]['tracks'][track_id]['artist_uri'][15:])\n",
    "            albums.append(slice['playlists'][playlist_id]['tracks'][track_id]['album_uri'][14:])\n",
    "            count_tracks += 1\n",
    "        playlist['tracks'] = tracks\n",
    "        playlist['artists'] = artists\n",
    "        playlist['albums'] = albums         \n",
    "        \n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    \n",
    "    with open('F:\\\\SpotifyPlaylistData\\\\MPD_Extract\\\\'+fname, 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count_names)\n",
    "print(count_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  traing with word2vec model to generate embedding vector for track\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            f = open(os.path.join(self.dirname,fname),encoding = 'utf8')\n",
    "            js = f.read()\n",
    "            f.close()\n",
    "            slice = json.loads(js)\n",
    "            for playlist_id in range(1000):\n",
    "                yield slice['playlists'][playlist_id]['tracks']\n",
    "\n",
    "sentences = MySentences('F:\\\\SpotifyPlaylistData\\\\MPD_Extract') # a memory-friendly iterator\n",
    "model = Word2Vec(size=40,alpha=0.025, window=5, min_count=1,sample=0.001, workers=8,\n",
    "                 min_alpha=0.025, sg=1,negative=5,iter=5)\n",
    "model.build_vocab(sentences)\n",
    "for epoch in range(10):\n",
    "    model.train(sentences, model.corpus_count, epochs=5)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "model.save('F:\\\\SpotifyPlaylistData\\\\word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  traing with doc2vec model to generate embedding vector for playlist name\n",
    "import gensim, logging\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class TaggedDoc(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            f = open(os.path.join(self.dirname, fname),encoding='utf8')\n",
    "            js = f.read()\n",
    "            f.close()\n",
    "            slice = json.loads(js)\n",
    "            for playlist_id in range(1000):\n",
    "                yield TaggedDocument(words=slice['playlists'][playlist_id]['tracks'],\n",
    "                                      tags=[slice['playlists'][playlist_id]['name'])\n",
    "\n",
    "model = Doc2Vec(dm=1, size=40, window=5, alpha=0.025, min_alpha=0.025, min_count=1, sample=0.001,\n",
    "                workers=4, iter=5, negative=5, dbow_words=1)\n",
    "TaggedDocs = TaggedDoc('F:\\\\SpotifyPlaylistData\\\\MPD_Extract')\n",
    "model.build_vocab(TaggedDocs)\n",
    "for epoch in range(10):\n",
    "    model.train(TaggedDocs,total_examples=1000000, epochs=5)\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "model.save('F:\\\\SpotifyPlaylistData\\\\doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating mapping between trackUrl and correponding indices\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def trackUrlGenerator(trackUrl2artistUrl,trackUrl2albumUrl):\n",
    "    count_playlists = 0\n",
    "    count_tracks = 0\n",
    "    for fname in os.listdir('F:\\\\SpotifyPlaylistData\\\\MPD\\\\data'):  \n",
    "        f = open(os.path.join('F:\\\\SpotifyPlaylistData\\\\MPD\\\\data', fname))\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            num_tracks = slice['playlists'][playlist_id]['num_tracks']\n",
    "            for track_id in range(num_tracks):\n",
    "                count_tracks += 1\n",
    "                trackUrl = slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:]\n",
    "                artistUrl = slice['playlists'][playlist_id]['tracks'][track_id]['artist_uri'][15:]\n",
    "                albumUrl = slice['playlists'][playlist_id]['tracks'][track_id]['album_uri'][14:]\n",
    "                trackUrl2artistUrl[trackUrl] = artistUrl\n",
    "                trackUrl2albumUrl[trackUrl] = albumUrl\n",
    "                yield trackUrl           \n",
    "            count_playlists += 1\n",
    "    print('count_playlists:',count_playlists)\n",
    "    print('count_tracks:',count_tracks)\n",
    "\n",
    "tracksUrl2idx = dict()\n",
    "trackUrl2artistUrl = dict()\n",
    "trackUrl2albumUrl = dict()\n",
    "tracksUrl_count = []\n",
    "trackUrlGen = trackUrlGenerator(trackUrl2artistUrl,trackUrl2albumUrl)\n",
    "counter = collections.Counter(trackUrlGen)\n",
    "tracksUrl_count.extend(counter.most_common(len(counter)))\n",
    "print('unique tracks -> len(tracksUrl_count):',len(tracksUrl_count))\n",
    "sum_tracks = 0\n",
    "for trackUrl, count in tracksUrl_count:\n",
    "    tracksUrl2idx[trackUrl] = len(tracksUrl2idx)\n",
    "    sum_tracks += count\n",
    "idx2tracksUrl = dict(zip(tracksUrl2idx.values(), tracksUrl2idx.keys()))\n",
    "print('total tracks(including repeating) -> sum_tracks:',sum_tracks)\n",
    "print('unique tracks -> len(tracksUrl2idx):',len(tracksUrl2idx))\n",
    "print('unique tracks -> len(idx2tracksUrl):',len(idx2tracksUrl))\n",
    "with open(\"tracksUrl2idx.txt\",\"wb\") as f:\n",
    "    pickle.dump(tracksUrl2idx,f)\n",
    "with open(\"idx2tracksUrl.txt\",\"wb\") as f:\n",
    "    pickle.dump(idx2tracksUrl,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating mapping between playlist name and integer indices\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "def nameGenerator():\n",
    "    count_playlists = 0\n",
    "    count_names = 0\n",
    "    for fname in os.listdir('F:\\\\SpotifyPlaylistData\\\\MPD_Extract'):  \n",
    "        f = open(os.path.join('F:\\\\SpotifyPlaylistData\\\\MPD_Extract', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            playlistName = slice['playlists'][playlist_id]['name']\n",
    "            for name in playlistName:\n",
    "                count_names += 1\n",
    "                yield name         \n",
    "            count_playlists += 1\n",
    "    print('count_playlists:',count_playlists)\n",
    "    print('count_names:',count_names)\n",
    "\n",
    "playlistName2idx = dict()\n",
    "playlistName_count = []\n",
    "playlistNameGen = nameGenerator()\n",
    "counter = collections.Counter(playlistNameGen)\n",
    "playlistName_count.extend(counter.most_common(len(counter)))\n",
    "print('len(counter):',len(counter))\n",
    "print('unique playlistNames -> len(playlistName_count):',len(playlistName_count))\n",
    "sum_playlistNames = 0\n",
    "for playlistName, count in playlistName_count:\n",
    "    playlistName2idx[playlistName] = len(playlistName2idx)\n",
    "    sum_playlistNames += count\n",
    "idx2playlistName = dict(zip(playlistName2idx.values(), playlistName2idx.keys()))\n",
    "print('total playlistName(including repeating) -> sum_playlistNames:',sum_playlistNames)\n",
    "print('unique playlistNames -> len(playlistName2idx):',len(playlistName2idx))\n",
    "print('unique playlistNames -> len(idx2playlistName):',len(idx2playlistName))\n",
    "with open(\"playlistName2idx.txt\",\"wb\") as f:\n",
    "    pickle.dump(playlistName2idx,f)\n",
    "with open(\"idx2playlistName.txt\",\"wb\") as f:\n",
    "    pickle.dump(idx2playlistName,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Indexing tracksUrl, artistUrl, albumUrl, playlist name, NO. of tracks. \n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"trackUrl2artistUrl.txt\",\"rb\") as f:\n",
    "    trackUrl2artistUrl = pickle.load(f)\n",
    "with open(\"trackUrl2albumUrl.txt\",\"rb\") as f:\n",
    "    trackUrl2albumUrl = pickle.load(f)\n",
    "with open(\"artistsUrl2idx.txt\",\"rb\") as f:\n",
    "    artistsUrl2idx = pickle.load(f)\n",
    "with open(\"albumsUrl2idx.txt\",\"rb\") as f:\n",
    "    albumsUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "artistsIdx = list()\n",
    "albumsIdx = list()\n",
    "count_tracks = 0\n",
    "for fname in os.listdir('F:\\\\SpotifyPlaylistData\\\\MPD_Extract'):\n",
    "    f = open(os.path.join('F:\\\\SpotifyPlaylistData\\\\MPD_Extract', fname),encoding='utf8')\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    print(fname)\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(1000):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "        del artistsIdx[:]\n",
    "        del albumsIdx[:]\n",
    "        \n",
    "        nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "        playlist['nameIdx'] = nameIdx\n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        artists = slice['playlists'][playlist_id]['artists']\n",
    "        albums = slice['playlists'][playlist_id]['albums']\n",
    "        tracks_len = len(tracks)\n",
    "        num_tracksIdx = num_tracks2idx[tracks_len]\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "            \n",
    "            artistIdx = artistsUrl2idx[trackUrl2artistUrl[tracks[track_id]]]\n",
    "            if artistIdx != artistsUrl2idx[artists[track_id]]:\n",
    "                print('artist doesn\\'t match!!')\n",
    "            else:\n",
    "                artistsIdx.append(artistIdx)\n",
    "            \n",
    "            albumIdx = albumsUrl2idx[trackUrl2albumUrl[tracks[track_id]]]\n",
    "            if albumIdx != albumsUrl2idx[albums[track_id]]:\n",
    "                print(\"album doesn't match!!\")\n",
    "            else:\n",
    "                albumsIdx.append(albumIdx)\n",
    "            count_tracks += 1\n",
    "        if slice['playlists'][playlist_id]['num_tracks'] != tracks_len:\n",
    "            print('num_tracks mismatch!!')\n",
    "        playlist['num_tracks'] = tracks_len\n",
    "        playlist['num_tracksIdx'] = num_tracksIdx\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        playlist['artistsIdx'] = artistsIdx\n",
    "        playlist['albumsIdx'] = albumsIdx\n",
    "        \n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('F:\\\\SpotifyPlaylistData\\\\MPD_Extract_Indices\\\\'+fname, 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating mapping between track and track's word2vec embedding features\n",
    "\n",
    "import pickle\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec.load('F:\\\\SpotifyPlaylistData\\\\word2vec.model')\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "\n",
    "tracksCount = 0    \n",
    "trackIdx2Features = dict() # key:string type, value:numpy type\n",
    "for tracksUrl in tracksUrl2idx:\n",
    "    trackIdx2Features[str(tracksUrl2idx[tracksUrl])] = model.wv[tracksUrl]\n",
    "    tracksCount += 1\n",
    "print('unique tracks:',tracksCount)\n",
    "print('len(trackIdx2Features):',len(trackIdx2Features))\n",
    "with open(\"trackIdx2Features.txt\",\"wb\") as f:\n",
    "    pickle.dump(trackIdx2Features,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating mapping between playlist name and its doc2vec embedding features\n",
    "\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model = Doc2Vec.load('F:\\\\SpotifyPlaylistData\\\\doc2vec.model')\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "\n",
    "namesCount = 0    \n",
    "nameIdx2Features = dict() # key:string type, value:numpy type\n",
    "for playlistName in playlistName2idx:\n",
    "    nameIdx2Features[str(playlistName2idx[playlistName])] = model.docvecs[playlistName]\n",
    "    namesCount += 1\n",
    "print('unique names:',namesCount)\n",
    "print('len(nameIdx2Features):',len(nameIdx2Features))\n",
    "with open(\"nameIdx2Features.txt\",\"wb\") as f:\n",
    "    pickle.dump(nameIdx2Features,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 1 as hdf5 file with fields ['num_tracksIdx'],['nameVec'],['tracksIdx'], coresponding to task category 1\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_1.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksIdx', (1000000,376),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_1.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)   \n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_1.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "            f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "            tracksIdx = slice['playlists'][playlist_id]['tracksIdx']       \n",
    "            \n",
    "            tracksIdx_l= np.asarray(tracksIdx)\n",
    "            tracksIdx_l_len = len(tracksIdx_l)\n",
    "            flag_376 = 0\n",
    "            for i in range(376//tracksIdx_l_len):\n",
    "                f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                flag_375 += tracksIdx_l_len\n",
    "            for i in range(tracksIdx_l_len):\n",
    "                if flag_376 < 376:\n",
    "                    f_data['tracksIdx'][count,flag_376] = tracksIdx_l[i]\n",
    "                    flag_376 += 1\n",
    "                else:\n",
    "                    break\n",
    "            count += 1\n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 2 as hdf5 file with fields ['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_2.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000000,40),dtype = 'float32') # one track\n",
    "    f.create_dataset('tracksIdx', (1000000,375),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_2.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_2.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "            f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "            tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "            f_data['tracksVec'][count,:] = trackIdx2Features[str(tracksIdx[0])]\n",
    "            tracksIdx_l= np.asarray(tracksIdx[1:])\n",
    "            tracksIdx_l_len = len(tracksIdx_l)\n",
    "            flag_375 = 0\n",
    "            for i in range(375//tracksIdx_l_len):\n",
    "                f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                flag_375 += tracksIdx_l_len\n",
    "            for i in range(tracksIdx_l_len):\n",
    "                if flag_375 < 375:\n",
    "                    f_data['tracksIdx'][count,flag_375] = tracksIdx_l[i]\n",
    "                    flag_375 += 1\n",
    "                else:\n",
    "                    break\n",
    "            count += 1\n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 3 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 3\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_3.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (994587,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (994587,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (994587,200),dtype = 'float32') # five tracks\n",
    "    f.create_dataset('tracksIdx', (994587,371),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_3.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_3.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([5,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 5:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(5):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()         \n",
    "                \n",
    "                tracksIdx_l= np.asarray(tracksIdx[5:])\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_371 = 0\n",
    "                for i in range(371//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_371 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_371 < 371:\n",
    "                        f_data['tracksIdx'][count,flag_371] = tracksIdx_l[i]\n",
    "                        flag_371 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 4 as hdf5 file with fields['num_tracksIdx'],['tracksVec'],['tracksIdx'],coresponding to task category 4\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_4.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (994587,1),dtype = 'int32')\n",
    "    f.create_dataset('tracksVec', (994587,200),dtype = 'float32') # five tracks\n",
    "    f.create_dataset('tracksIdx', (994587,371),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_4.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_4.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([5,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 5:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(5):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()        \n",
    "                \n",
    "                tracksIdx_l= np.asarray(tracksIdx[5:])\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_371 = 0\n",
    "                for i in range(371//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_371 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_371 < 371:\n",
    "                        f_data['tracksIdx'][count,flag_371] = tracksIdx_l[i]\n",
    "                        flag_371 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 5 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 5\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_5.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (953325,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (953325,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (953325,400),dtype = 'float32') # ten tracks\n",
    "    f.create_dataset('tracksIdx', (953325,366),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_5.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_5.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([10,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 10:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(10):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                \n",
    "                tracksIdx_l= np.asarray(tracksIdx[10:])\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_366 = 0\n",
    "                for i in range(366//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_366 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_366 < 366:\n",
    "                        f_data['tracksIdx'][count,flag_366] = tracksIdx_l[i]\n",
    "                        flag_366 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 6 as hdf5 file with fields['num_tracksIdx'],['tracksVec'],['tracksIdx'],coresponding to task category 6\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_6.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (953325,1),dtype = 'int32')\n",
    "    f.create_dataset('tracksVec', (953325,400),dtype = 'float32') # ten tracks\n",
    "    f.create_dataset('tracksIdx', (953325,366),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_6.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_6.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([10,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 10:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(10):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()        \n",
    "                \n",
    "                tracksIdx_l= np.asarray(tracksIdx[10:])\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_366 = 0\n",
    "                for i in range(366//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_366 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_366 < 366:\n",
    "                        f_data['tracksIdx'][count,flag_366] = tracksIdx_l[i]\n",
    "                        flag_366 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 7 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 7\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_7.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (754348,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (754348,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (754348,1000),dtype = 'float32') # twenty five tracks\n",
    "    f.create_dataset('tracksIdx', (754348,351),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_7.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_7.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([25,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 25:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(25):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                \n",
    "                tracksIdx_l= np.asarray(tracksIdx[25:])\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_351 = 0\n",
    "                for i in range(351//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_351 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_351 < 351:\n",
    "                        f_data['tracksIdx'][count,flag_351] = tracksIdx_l[i]\n",
    "                        flag_351 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 8 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 8\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_8.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (754348,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (754348,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (754348,1000),dtype = 'float32') # random twenty five tracks\n",
    "    f.create_dataset('tracksIdx', (754348,351),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_8.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_8.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_0 = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([25,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 25:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                \n",
    "                tracksIdx_permutation = np.random.permutation(tracksIdx)\n",
    "                rand_tracksIdx = tracksIdx_permutation[:25]\n",
    "                for i in range(25):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(rand_tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                \n",
    "                tracksIdx_l = tracksIdx_permutation[25:]\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                if tracksIdx_l_len == 0:\n",
    "                    print('tracksIdx:',tracksIdx)\n",
    "                    print('rand_tracksIdx:',rand_tracksIdx)\n",
    "                    print('tracksIdx_l:',tracksIdx_l)\n",
    "                    count_0 += 1\n",
    "                    break\n",
    "                flag_351 = 0\n",
    "                for i in range(351//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_351 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_351 < 351:\n",
    "                        f_data['tracksIdx'][count,flag_351] = tracksIdx_l[i]\n",
    "                        flag_351 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1\n",
    "                \n",
    "print('count_0:',count_0)\n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 9 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 9\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_9.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (216482,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (216482,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (216482,4000),dtype = 'float32') # one hundred tracks\n",
    "    f.create_dataset('tracksIdx', (216482,276),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_9.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_9.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([100,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 100:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(100):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                \n",
    "                tracksIdx_l= np.asarray(tracksIdx[100:])\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_276 = 0\n",
    "                for i in range(276//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_276 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_276 < 276:\n",
    "                        f_data['tracksIdx'][count,flag_276] = tracksIdx_l[i]\n",
    "                        flag_276 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate datset 10 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 10\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_10.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (216482,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (216482,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (216482,4000),dtype = 'float32') # random one hundred tracks\n",
    "    f.create_dataset('tracksIdx', (216482,276),dtype = 'int32',fillvalue=-1)\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_10.hdf5','r') as f:\n",
    "    print(f['tracksIdx'][0,:].shape)\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('F:/SpotifyPlaylistData/Dataset_10.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    count_playlists = 0\n",
    "    tracksVec_arr = np.zeros([100,40],dtype=np.float32)\n",
    "    for fname in os.listdir('F:/SpotifyPlaylistData/MPD_Extract_Indices'):  \n",
    "        f = open(os.path.join('F:/SpotifyPlaylistData/MPD_Extract_Indices', fname),encoding='utf8')\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        print(fname)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            count_playlists += 1\n",
    "            if slice['playlists'][playlist_id]['num_tracks'] > 100:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                \n",
    "                tracksIdx_permutation = np.random.permutation(tracksIdx)\n",
    "                rand_tracksIdx = tracksIdx_permutation[:100]\n",
    "                for i in range(100):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(rand_tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                \n",
    "                tracksIdx_l = tracksIdx_permutation[100:]\n",
    "                tracksIdx_l_len = len(tracksIdx_l)\n",
    "                flag_276 = 0\n",
    "                for i in range(276//tracksIdx_l_len):\n",
    "                    f_data['tracksIdx'][count,i*tracksIdx_l_len:(i+1)*tracksIdx_l_len] = tracksIdx_l\n",
    "                    flag_276 += tracksIdx_l_len\n",
    "                for i in range(tracksIdx_l_len):\n",
    "                    if flag_276 < 276:\n",
    "                        f_data['tracksIdx'][count,flag_276] = tracksIdx_l[i]\n",
    "                        flag_276 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)\n",
    "print('count_playlists:',count_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 1\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input, Embedding, Dense,concatenate,Reshape\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 1000000\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_1.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                count_playlists += 1\n",
    "                yield [num_tracksIdx, name_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "x = concatenate([num_tracksIdx_em, name_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "\n",
    "model.save('task1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 2\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input, Embedding, Dense,concatenate,Reshape\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 1000000\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_2.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                count_playlists += 1\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(40,), name='tracks_vec_input')\n",
    "tracks_output = Dense(50, activation='tanh', name='tracks_output')(tracks_vec_input)\n",
    "\n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "\n",
    "model.save('task2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 3\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 994587\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_3.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][994550:994587,:]\n",
    "            name_vec = f['nameVec'][994550:994587,:]\n",
    "            tracks_vec = f['tracksVec'][994550:994587,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][994550:994587,:])\n",
    "            yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(200,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([5,40])(tracks_vec_input)\n",
    "tracks_output = GRU(60,input_shape=(5,40))(tracks_vec)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 4\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 994587\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_4.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][994550:994587,:]\n",
    "            tracks_vec = f['tracksVec'][994550:994587,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][994550:994587,:])\n",
    "            yield [num_tracksIdx, tracks_vec], tracksIdx_encoding\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "tracks_vec_input = Input(shape=(200,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([5,40])(tracks_vec_input)\n",
    "tracks_output = GRU(60,input_shape=(5,40))(tracks_vec)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 5\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 953325\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_5.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][953300:953325,:]\n",
    "            name_vec = f['nameVec'][953300:953325,:]\n",
    "            tracks_vec = f['tracksVec'][953300:953325,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][953300:953325,:])\n",
    "            yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(400,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([10,40])(tracks_vec_input)\n",
    "tracks_output = GRU(80,input_shape=(10,40))(tracks_vec)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 6\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 953325\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_6.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][953300:953325,:]\n",
    "            tracks_vec = f['tracksVec'][953300:953325,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][953300:953325,:])\n",
    "            yield [num_tracksIdx, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "tracks_vec_input = Input(shape=(400,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([10,40])(tracks_vec_input)\n",
    "tracks_output = GRU(80,input_shape=(10,40))(tracks_vec)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 7\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 754348\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_7.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][754300:754348,:]\n",
    "            name_vec = f['nameVec'][754300:754348,:]\n",
    "            tracks_vec = f['tracksVec'][754300:754348,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][754300:754348,:])\n",
    "            yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(1000,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([25,40])(tracks_vec_input)\n",
    "tracks_output = GRU(100,input_shape=(25,40))(tracks_vec)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 8\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU,Conv2D,MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 754348\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_8.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][754300:754348,:]\n",
    "            name_vec = f['nameVec'][754300:754348,:]\n",
    "            tracks_vec = f['tracksVec'][754300:754348,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][754300:754348,:])\n",
    "            yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(1000,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([25,40,1])(tracks_vec_input)\n",
    "\n",
    "tracks_vec_Conv1 = Conv2D(64,(1,40),activation = 'relu',padding = 'valid',input_shape = (25,40,1))(tracks_vec)\n",
    "tracks_vec_Conv2 = Conv2D(128,(1,1),activation = 'relu',padding = 'valid',input_shape = (25,1,64))(tracks_vec_Conv1)\n",
    "tracks_vec_MaxPool = MaxPool2D((25,1),padding = 'valid')(tracks_vec_Conv2)\n",
    "tracks_output = Reshape([128,])(tracks_vec_MaxPool)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 9\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 216482\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_9.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][216450:216482,:]\n",
    "            name_vec = f['nameVec'][216450:216482,:]\n",
    "            tracks_vec = f['tracksVec'][216450:216482,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][216450:216482,:])\n",
    "            yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(4000,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([100,40])(tracks_vec_input)\n",
    "tracks_output = GRU(200,input_shape=(100,40))(tracks_vec)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train neural model for task category 10\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.layers import Input,Embedding,Dense,concatenate,Reshape,GRU,Conv2D,MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "vocab = [(n,) for n in range(2262292)]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(vocab)\n",
    "\n",
    "num_train = 216482\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "steps_per_epoch = num_train//batch_size\n",
    "def inputGen(mlb):\n",
    "    count_playlists = 0\n",
    "    with h5py.File('C:/Users/zwang10/Research/Dataset_10.hdf5','r') as f:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(steps_per_epoch):\n",
    "                num_tracksIdx = f['num_tracksIdx'][i*batch_size:(i+1)*batch_size,:]\n",
    "                name_vec = f['nameVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracks_vec = f['tracksVec'][i*batch_size:(i+1)*batch_size,:]\n",
    "                tracksIdx_encoding = mlb.transform(f['tracksIdx'][i*batch_size:(i+1)*batch_size,:])\n",
    "                yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "            num_tracksIdx = f['num_tracksIdx'][216450:216482,:]\n",
    "            name_vec = f['nameVec'][216450:216482,:]\n",
    "            tracks_vec = f['tracksVec'][216450:216482,:]\n",
    "            tracksIdx_encoding = mlb.transform(f['tracksIdx'][216450:216482,:])\n",
    "            yield [num_tracksIdx, name_vec, tracks_vec], tracksIdx_encoding\n",
    "\n",
    "    \n",
    "inputgenerator = inputGen(mlb)\n",
    "num_tracksIdx_input = Input(shape=(1,),dtype='int32',name='num_tracksIdx_input')\n",
    "num_tracksIdx = Embedding(input_dim=247,output_dim=10,input_length=1)(num_tracksIdx_input)\n",
    "num_tracksIdx_em = Reshape([10,])(num_tracksIdx)\n",
    "\n",
    "name_vec_input = Input(shape=(40,), name='name_vec_input')\n",
    "name_output = Dense(50, activation='tanh', name='name_output')(name_vec_input)\n",
    "\n",
    "tracks_vec_input = Input(shape=(4000,), name='tracks_vec_input')\n",
    "tracks_vec = Reshape([100,40,1])(tracks_vec_input)\n",
    "\n",
    "tracks_vec_Conv1 = Conv2D(128,(1,40),activation = 'relu',padding = 'valid',input_shape = (100,40,1))(tracks_vec)\n",
    "tracks_vec_Conv2 = Conv2D(200,(1,1),activation = 'relu',padding = 'valid',input_shape = (100,1,64))(tracks_vec_Conv1)\n",
    "tracks_vec_MaxPool = MaxPool2D((100,1),padding = 'valid')(tracks_vec_Conv2)\n",
    "tracks_output = Reshape([200,])(tracks_vec_MaxPool)\n",
    "    \n",
    "x = concatenate([num_tracksIdx_em, name_output,tracks_output])\n",
    "x_dense = Dense(64, activation='relu')(x)\n",
    "output = Dense(2262292, activation='sigmoid',name='output')(x_dense)\n",
    "model = Model(inputs=[num_tracksIdx_input, name_vec_input,tracks_vec_input], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit_generator(inputgenerator,steps_per_epoch=steps_per_epoch+1, epochs=epochs)\n",
    "\n",
    "model.save('task10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 1: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples from challenge datset\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 0:\n",
    "            pid = slice['playlists'][playlist_id]['pid']\n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            if 'name' in slice['playlists'][playlist_id]:\n",
    "                title = slice['playlists'][playlist_id]['name']\n",
    "                playlist_title = ' '.join(title.lower().split())\n",
    "                playlist['name'] = playlist_title\n",
    "                count += 1\n",
    "            playlist['pid'] = pid\n",
    "            playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "            playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "            playlist['tracks'] = tracks\n",
    "            playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case1.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 2: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 1:\n",
    "            pid = slice['playlists'][playlist_id]['pid']\n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            if 'name' in slice['playlists'][playlist_id]:\n",
    "                title = slice['playlists'][playlist_id]['name']\n",
    "                playlist_title = ' '.join(title.lower().split())\n",
    "                playlist['name'] = playlist_title\n",
    "                count += 1\n",
    "            playlist['pid'] = pid\n",
    "            playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "            playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "            playlist['tracks'] = tracks\n",
    "            playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case2.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 3: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 5 and 'name' in slice['playlists'][playlist_id]:\n",
    "            pid = slice['playlists'][playlist_id]['pid']\n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            if 'name' in slice['playlists'][playlist_id]:\n",
    "                title = slice['playlists'][playlist_id]['name']\n",
    "                playlist_title = ' '.join(title.lower().split())\n",
    "                playlist['name'] = playlist_title\n",
    "                count += 1\n",
    "            playlist['pid'] = pid\n",
    "            playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "            playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "            playlist['tracks'] = tracks\n",
    "            playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case3.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 4: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 5 and 'name' not in slice['playlists'][playlist_id]:\n",
    "            pid = slice['playlists'][playlist_id]['pid']\n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            if 'name' in slice['playlists'][playlist_id]:\n",
    "                title = slice['playlists'][playlist_id]['name']\n",
    "                playlist_title = ' '.join(title.lower().split())\n",
    "                playlist['name'] = playlist_title\n",
    "            else:\n",
    "                count += 1\n",
    "            playlist['pid'] = pid\n",
    "            playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "            playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "            playlist['tracks'] = tracks\n",
    "            playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case4.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 5: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 10 and 'name' in slice['playlists'][playlist_id]:\n",
    "            pid = slice['playlists'][playlist_id]['pid']\n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            if 'name' in slice['playlists'][playlist_id]:\n",
    "                title = slice['playlists'][playlist_id]['name']\n",
    "                playlist_title = ' '.join(title.lower().split())\n",
    "                playlist['name'] = playlist_title\n",
    "                count += 1\n",
    "            playlist['pid'] = pid\n",
    "            playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "            playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "            playlist['tracks'] = tracks\n",
    "            playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case5.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 6: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 10 and 'name' not in slice['playlists'][playlist_id]:\n",
    "            pid = slice['playlists'][playlist_id]['pid']\n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "            if 'name' in slice['playlists'][playlist_id]:\n",
    "                title = slice['playlists'][playlist_id]['name']\n",
    "                playlist_title = ' '.join(title.lower().split())\n",
    "                playlist['name'] = playlist_title\n",
    "            else:\n",
    "                count += 1\n",
    "            playlist['pid'] = pid\n",
    "            playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "            playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "            playlist['tracks'] = tracks\n",
    "            playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case6.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 7: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 25:\n",
    "            orderedPos = True \n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                if slice['playlists'][playlist_id]['tracks'][track_id]['pos'] != track_id:\n",
    "                    orderedPos = False\n",
    "                    break\n",
    "            if orderedPos == True:\n",
    "                pid = slice['playlists'][playlist_id]['pid']\n",
    "                for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                    tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "                if 'name' in slice['playlists'][playlist_id]:\n",
    "                    title = slice['playlists'][playlist_id]['name']\n",
    "                    playlist_title = ' '.join(title.lower().split())\n",
    "                    playlist['name'] = playlist_title\n",
    "                    count += 1\n",
    "                playlist['pid'] = pid\n",
    "                playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "                playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "                playlist['tracks'] = tracks\n",
    "                playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case7.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 8: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 25:\n",
    "            orderedPos = True \n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                if slice['playlists'][playlist_id]['tracks'][track_id]['pos'] != track_id:\n",
    "                    orderedPos = False\n",
    "                    break\n",
    "            if orderedPos == False:\n",
    "                pid = slice['playlists'][playlist_id]['pid']\n",
    "                for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                    tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "                if 'name' in slice['playlists'][playlist_id]:\n",
    "                    title = slice['playlists'][playlist_id]['name']\n",
    "                    playlist_title = ' '.join(title.lower().split())\n",
    "                    playlist['name'] = playlist_title \n",
    "                    count += 1\n",
    "                playlist['pid'] = pid\n",
    "                playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "                playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "                playlist['tracks'] = tracks\n",
    "                playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case8.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 9: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 100:\n",
    "            orderedPos = True \n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                if slice['playlists'][playlist_id]['tracks'][track_id]['pos'] != track_id:\n",
    "                    orderedPos = False\n",
    "                    break\n",
    "            if orderedPos == True:\n",
    "                pid = slice['playlists'][playlist_id]['pid']\n",
    "                for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                    tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "                if 'name' in slice['playlists'][playlist_id]:\n",
    "                    title = slice['playlists'][playlist_id]['name']\n",
    "                    playlist_title = ' '.join(title.lower().split())\n",
    "                    playlist['name'] = playlist_title \n",
    "                    count += 1\n",
    "                playlist['pid'] = pid\n",
    "                playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "                playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "                playlist['tracks'] = tracks\n",
    "                playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case9.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  category 10: preProcess playlist name and Extract trackurl, num_tracks, name, num_samples\n",
    "import json\n",
    "import string\n",
    "import copy\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracks = list()\n",
    "count = 0  \n",
    "with open('C:\\\\potifyPlaylistData\\\\ChallengeData\\\\challenge_set.json') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracks[:]\n",
    "        if slice['playlists'][playlist_id]['num_samples'] == 100:\n",
    "            orderedPos = True \n",
    "            for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                if slice['playlists'][playlist_id]['tracks'][track_id]['pos'] != track_id:\n",
    "                    orderedPos = False\n",
    "                    break\n",
    "            if orderedPos == False:\n",
    "                pid = slice['playlists'][playlist_id]['pid']\n",
    "                for track_id in range(len(slice['playlists'][playlist_id]['tracks'])):\n",
    "                    tracks.append(slice['playlists'][playlist_id]['tracks'][track_id]['track_uri'][14:])\n",
    "                if 'name' in slice['playlists'][playlist_id]:\n",
    "                    title = slice['playlists'][playlist_id]['name']\n",
    "                    playlist_title = ' '.join(title.lower().split())\n",
    "                    playlist['name'] = playlist_title \n",
    "                    count += 1\n",
    "                playlist['pid'] = pid\n",
    "                playlist['num_tracks'] =  slice['playlists'][playlist_id]['num_tracks']\n",
    "                playlist['num_samples'] =  slice['playlists'][playlist_id]['num_samples']\n",
    "                playlist['tracks'] = tracks\n",
    "                playlists.append(copy.deepcopy(playlist))\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case10.json', 'w', encoding='utf8') as outfile:        \n",
    "    outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# speicial playlist name\n",
    "spe_name = ['je taime','raphop','itslit','universal stereo','gott','deporte','jakes playlist','mumford sons wilder mind deluxe',\n",
    "           'we cant stop','throwbackpop','gezellig','throwbackrap','rythm','mixtape2']\n",
    "simil_name = [\"je t'aime\",'rap hop','its lit','universal','god','sport',\"jake's playlist\",'mumford & sons – wilder mind',\n",
    "           \"we can't stop\",'throwback pop','cozy','throwback rap','rhythm','mixtape 2']\n",
    "dict_speName = dict()\n",
    "for i in range(len(spe_name)):\n",
    "    dict_speName[spe_name[i]] = simil_name[i]\n",
    "with open(\"dict_speName.txt\",\"wb\") as f:\n",
    "    pickle.dump(dict_speName,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 1 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case1.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case1.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 2 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case2.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 1 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case2.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 3 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case3.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 5 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case3.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 4 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case4.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "        \n",
    "        if 'name' in slice['playlists'][playlist_id]:\n",
    "            name = slice['playlists'][playlist_id]['name']\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "                if name in playlistName2idx:\n",
    "                    nameIdx = playlistName2idx[name]\n",
    "                    playlist['nameIdx'] = nameIdx\n",
    "                    count_name += 1\n",
    "                else:\n",
    "                    nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                    playlist['nameIdx'] = nameIdx\n",
    "                    count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 5 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case4.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 5 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case5.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 10 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case5.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 6 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case6.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "        \n",
    "        if 'name' in slice['playlists'][playlist_id]:\n",
    "            name = slice['playlists'][playlist_id]['name']\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "                if name in playlistName2idx:\n",
    "                    nameIdx = playlistName2idx[name]\n",
    "                    playlist['nameIdx'] = nameIdx\n",
    "                    count_name += 1\n",
    "                else:\n",
    "                    nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                    playlist['nameIdx'] = nameIdx\n",
    "                    count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 10 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case6.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 7 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case7.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 25 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case7.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 8 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case8.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 25 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case8.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 9 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case9.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 100 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case9.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  indexing track,playlist name in challenge datset for category 10 with integers\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"playlistName2idx.txt\",\"rb\") as f:\n",
    "    playlistName2idx = pickle.load(f)\n",
    "with open(\"tracksUrl2idx.txt\",\"rb\") as f:\n",
    "    tracksUrl2idx = pickle.load(f)\n",
    "with open(\"num_tracks2idx.txt\",\"rb\") as f:\n",
    "    num_tracks2idx = pickle.load(f)\n",
    "with open(\"dict_speName.txt\",\"rb\") as f:\n",
    "    dict_speName = pickle.load(f)\n",
    "\n",
    "playlists_MPD = dict()\n",
    "playlists = list()\n",
    "playlist = dict()\n",
    "tracksIdx = list()\n",
    "count_playlist = 0\n",
    "count_name = 0\n",
    "with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_case10.json',encoding='utf8') as f:\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    slice = json.loads(js)\n",
    "    playlists_MPD.clear()\n",
    "    playlists.clear()\n",
    "    for playlist_id in range(len(slice['playlists'])):\n",
    "        playlist.clear()\n",
    "        del tracksIdx[:]\n",
    "          \n",
    "        name = slice['playlists'][playlist_id]['name']\n",
    "        if name in playlistName2idx:\n",
    "            nameIdx = playlistName2idx[slice['playlists'][playlist_id]['name']]\n",
    "            playlist['nameIdx'] = nameIdx\n",
    "            count_name += 1\n",
    "        else:\n",
    "            name = ' '.join(''.join(e for e in name if e.isalnum() or e == ' ').split())\n",
    "            if name in playlistName2idx:\n",
    "                nameIdx = playlistName2idx[name]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "            else:\n",
    "                nameIdx = playlistName2idx[dict_speName[name]]\n",
    "                playlist['nameIdx'] = nameIdx\n",
    "                count_name += 1\n",
    "    \n",
    "        playlist['num_tracks'] = slice['playlists'][playlist_id][\"num_tracks\"]                     \n",
    "        playlist['num_tracksIdx'] = num_tracks2idx[slice['playlists'][playlist_id][\"num_tracks\"]] \n",
    "        \n",
    "        pid = slice['playlists'][playlist_id]['pid']\n",
    "        playlist['pid'] = pid\n",
    "        \n",
    "        tracks = slice['playlists'][playlist_id]['tracks']\n",
    "        tracks_len = len(tracks)\n",
    "\n",
    "        for track_id in range(tracks_len):\n",
    "            tracksIdx.append(tracksUrl2idx[tracks[track_id]])\n",
    "        playlist['tracksIdx'] = tracksIdx\n",
    "        \n",
    "        if 100 != tracks_len:\n",
    "            print(\"num_tracks doesn't match!!\")\n",
    "\n",
    "        playlists.append(copy.deepcopy(playlist))\n",
    "        count_playlist += 1\n",
    "    playlists_MPD['playlists'] = playlists\n",
    "    with open('C:\\\\SpotifyPlaylistData\\\\ChallengeData\\\\challenge_set_Extract_Indices_case10.json', 'w', encoding='utf8') as outfile:        \n",
    "        outfile.write(json.dumps(playlists_MPD, indent=4, sort_keys=True, ensure_ascii=False,separators=(',', ': ')))\n",
    "print('count_playlist:',count_playlist)\n",
    "print('count_name:',count_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 1 as hdf5 file with fields ['num_tracksIdx'],['nameVec'],['tracksIdx'], coresponding to task category 1\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_1.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_1.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case1.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if 'nameIdx' in slice['playlists'][playlist_id]:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 2 as hdf5 file with fields ['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 2\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/Users/ZhenWang/SpotifyPlaylistData/ChallengeData/challenge_Dataset_2.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/Users/ZhenWang/SpotifyPlaylistData/ChallengeData/challenge_Dataset_2.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    with open('C:/Users/ZhenWang/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case2.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 1 and 'nameIdx' in slice['playlists'][playlist_id]:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                f_data['tracksVec'][count,:] = trackIdx2Features[str(tracksIdx[0])]\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 3 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 3\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_3.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,200),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_3.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([5,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case3.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 5 and 'nameIdx' in slice['playlists'][playlist_id]:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(5):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 4 as hdf5 file with fields['num_tracksIdx'],['tracksVec'],['tracksIdx'],coresponding to task category 4\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/potifyPlaylistData/ChallengeData/challenge_Dataset_4.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('tracksVec', (1000,200),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_4.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([5,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case4.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 5 and 'nameIdx' not in slice['playlists'][playlist_id]:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(5):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 5 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 5\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_5.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,400),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_5.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([10,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case5.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 10 and 'nameIdx' in slice['playlists'][playlist_id]:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(10):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 6 as hdf5 file with fields['num_tracksIdx'],['tracksVec'],['tracksIdx'],coresponding to task category 6\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_6.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('tracksVec', (1000,400),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_6.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([10,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case6.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 10 and 'nameIdx' not in slice['playlists'][playlist_id]:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(10):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 7 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 7\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_7.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,1000),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_7.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([25,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case7.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 25:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(25):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 8 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 8\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_8.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,1000),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_8.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([25,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case8.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 25:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(25):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 9 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 9\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_9.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,4000),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_9.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([100,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case9.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 100:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(100):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genrate challenge datset 10 as hdf5 file with fields['num_tracksIdx'],['nameVec'],['tracksVec'],['tracksIdx'],coresponding to task category 10\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_10.hdf5','w') as f:\n",
    "    f.create_dataset('num_tracksIdx', (1000,1),dtype = 'int32')\n",
    "    f.create_dataset('nameVec', (1000,40),dtype = 'float32')\n",
    "    f.create_dataset('tracksVec', (1000,4000),dtype = 'float32')\n",
    "    f.create_dataset('pid', (1000,1),dtype = 'int32')\n",
    "    \n",
    "with open(\"nameIdx2Features.txt\",\"rb\") as f:\n",
    "    nameIdx2Features = pickle.load(f)\n",
    "with open(\"trackIdx2Features.txt\",\"rb\") as f:\n",
    "    trackIdx2Features = pickle.load(f)\n",
    "\n",
    "with h5py.File('C:/SpotifyPlaylistData/ChallengeData/challenge_Dataset_10.hdf5','a') as f_data:\n",
    "    count = 0\n",
    "    tracksVec_arr = np.zeros([100,40],dtype=np.float32)\n",
    "    with open('C:/SpotifyPlaylistData/ChallengeData/challenge_set_Extract_Indices_case10.json',encoding='utf8') as f:\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        slice = json.loads(js)\n",
    "        for playlist_id in range(len(slice['playlists'])):\n",
    "            if len(slice['playlists'][playlist_id]['tracksIdx']) == 100:\n",
    "                f_data['num_tracksIdx'][count,:] = slice['playlists'][playlist_id]['num_tracksIdx']\n",
    "                f_data['nameVec'][count,:] = nameIdx2Features[str(slice['playlists'][playlist_id]['nameIdx'])]\n",
    "                tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "                for i in range(100):\n",
    "                    tracksVec_arr[i,:] = trackIdx2Features[str(tracksIdx[i])]\n",
    "                f_data['tracksVec'][count,:] = tracksVec_arr.flatten()\n",
    "                f_data['pid'][count,:] = slice['playlists'][playlist_id]['pid']\n",
    "                count += 1 \n",
    "            \n",
    "print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 1 \n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task1.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "fields = [\"FishInMedi\",\"main\",\"Results Submission\",\"zwang10@uwyo.edu\"]\n",
    "count = 0\n",
    "with h5py.File('C:/Users/zwang10/Research/challenge_Dataset_1.hdf5','r') as f,open('C:/Users/zwang10/Research/challenge_1.csv','w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "    for playlist_id in range(0,1000):\n",
    "        num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "        name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "        pid = f['pid'][playlist_id,:][0]\n",
    "        row = [pid]\n",
    "\n",
    "        predictions = model.predict([num_tracksIdx,name_vec])\n",
    "    \n",
    "        for trackIdx in np.flip(np.argsort(predictions)[0,-500:],axis=-1):\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "        csvwriter.writerow(row)\n",
    "        count += 1\n",
    "print('count:',count)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 2 \n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task2.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_2.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_2.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case2.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,40])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-501:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 3 \n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task3.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_3.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_3.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case3.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,200])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-505:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 4\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task4.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_4.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_4.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case4.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,200])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-505:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 5\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task5.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_5.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_5.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case5.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,400])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-510:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 6\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task6.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_6.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_6.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case6.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,400])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-510:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 7\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task7.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_7.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_7.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case7.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,1000])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-525:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 8\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task8.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_8.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_8.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case8.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,1000])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-525:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 9\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task9.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_9.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_9.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case9.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,4000])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-600:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predicted results for category 10\n",
    "\n",
    "from keras.models import load_model #Library for loading saved models.\n",
    "import json\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "model = load_model('./plylistContinuation/task10.h5')\n",
    "model.summary()\n",
    "\n",
    "with open(\"C:/Users/zwang10/Research/idx2tracksUrl.txt\",\"rb\") as f:\n",
    "    idx2tracksUrl = pickle.load(f)\n",
    "    \n",
    "count = 0\n",
    "count_misMatch = 0\n",
    "f = h5py.File('C:/Users/zwang10/Research/challenge_Dataset_10.hdf5','r')\n",
    "csvfile = open('C:/Users/zwang10/Research/challenge_10.csv','w')\n",
    "fj = open('C:/Users/zwang10/Research/challenge_set_Extract_Indices_case10.json',encoding='utf8')\n",
    "js = fj.read()\n",
    "slice = json.loads(js)\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for playlist_id in range(0,1000):\n",
    "    print('playlist_id -',playlist_id)\n",
    "    num_tracksIdx = f['num_tracksIdx'][playlist_id,:].reshape([-1,1])\n",
    "    name_vec = f['nameVec'][playlist_id,:].reshape([-1,40])\n",
    "    tracks_vec = f['tracksVec'][playlist_id,:].reshape([-1,4000])\n",
    "    pid = f['pid'][playlist_id,:][0]\n",
    "    row = [pid]\n",
    "    predictions = model.predict([num_tracksIdx,name_vec,tracks_vec])\n",
    "    \n",
    "    js_pid = slice['playlists'][playlist_id]['pid']\n",
    "    if js_pid != pid:\n",
    "        print(\"Does't match!\")\n",
    "        count_misMatch += 1\n",
    "    tracksIdx = slice['playlists'][playlist_id]['tracksIdx']\n",
    "    break_flag = 0\n",
    "    for trackIdx in np.flip(np.argsort(predictions)[0,-600:],axis=-1):\n",
    "        if trackIdx not in tracksIdx:\n",
    "            row.append('spotify:track:'+idx2tracksUrl[trackIdx])\n",
    "            break_flag += 1\n",
    "            count += 1\n",
    "        if break_flag == 500:\n",
    "            break\n",
    "    csvwriter.writerow(row)\n",
    "print('count:',count)\n",
    "print('misMatch:',count_misMatch)\n",
    "print('Done!')\n",
    "f.close()\n",
    "csvfile.close()\n",
    "fj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge into a complete submission\n",
    "\n",
    "import csv\n",
    "\n",
    "count = 0\n",
    "with open('C:/Users/zwang10/Research/submission.csv','w',newline='') as sub, open('C:/Users/zwang10/Research/challenge_1.csv','r') as par1:\n",
    "    csvreader = csv.reader(par1)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "\n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_2.csv','r') as par2:\n",
    "    csvreader = csv.reader(par2)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "\n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_3.csv','r') as par3:\n",
    "    csvreader = csv.reader(par3)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "        \n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_4.csv','r') as par4:\n",
    "    csvreader = csv.reader(par4)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "        \n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_5.csv','r') as par5:\n",
    "    csvreader = csv.reader(par5)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "        \n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_6.csv','r') as par6:\n",
    "    csvreader = csv.reader(par6)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "\n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_7.csv','r') as par7:\n",
    "    csvreader = csv.reader(par7)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "        \n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_8.csv','r') as par8:\n",
    "    csvreader = csv.reader(par8)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "        \n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_9.csv','r') as par9:\n",
    "    csvreader = csv.reader(par9)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "        \n",
    "with open('C:/Users/zwang10/Research/submission.csv','a',newline='') as sub, open('C:/Users/zwang10/Research/challenge_10.csv','r') as par10:\n",
    "    csvreader = csv.reader(par10)\n",
    "    writer = csv.writer(sub)\n",
    "    for row in csvreader:\n",
    "        writer.writerow(row)\n",
    "        count += 1\n",
    "print('count:',count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
